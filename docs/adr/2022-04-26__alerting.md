# Alerting in OpMon

* Author: Anna Scholtz
* Date: 2022-04-26
* Status: Accepting Feedback (Feedback from DS is especially welcome)

## Introduction

Users of OpMon want to be able to see critical changes and optionally get notified whenever they occur. This ADR describes how OpMon will provide the ability to define different monitors, how they will be implemented and how users can subscribe to getting email notifications.

As an example, of what the operational monitoring dashboards currently look like see: https://mozilla.cloud.looker.com/dashboards/operational_monitoring::bug_1715474_rollout_yandex_sponsored_tile_rollout_release_89_100?Percentile=50
The dashboards have a tile for each metric which shows the values of all available branches for the percentile currently selected as filter.

## Constraints

There are a couple of constraints when checking for critical changes in OpMon:
* **Cost:** We will want to keep the cost low since there will be a lot of dashboards (for example, we create one for each rollout)
* **Computation Speed:** The operational monitoring dashboards get updated on a daily basis and there are usually dozens of dashboards that need to get updated. Ideally, the checks for each dashboard and day finish within minutes rather than hours.
* **Minimize False Positives:** Sending alerts when nothing is wrong too many times results in users simply ignoring alerts at some point

## Monitor Types

When talking to users there are a few different cases when they would like to receive an alert:
* **Large differences between branches:** Whenever the confidence intervals of different branches for a specific metric no longer overlap, it indicates that there is potentially some significant difference.
* **Thresholds:** Comparing the values of a metric to a user-defined threshold.
* **Deviation from historical data:** Detect anomalous behaviour of a metric based on previously collected data.

### Large differences between branches

The OpMon dashboards show the values for specific metrics as a line chart with confidence intervals. Each line represents the metric values for a different branch. Whenever the confidence intervals of the branches do not overlap, it is considered a critical change. See:

<img src="./images/alerting_branch_differences.png" width="500"/>

These checks need to be manually configured for specific metrics/probes. The OpMon configuration files will be extended to something like this:

```toml
[alert]

[alert.crashes] # new alert
type = "ci"
probes = [      # list of metrics to check
    "startup_crashes", 
    "main_crashes", 
    "oom_crashes"
]
percentiles = [50, 90]  # percentiles that should be considered
```


### Thresholds

In some cases the expected value of a metric is known and any large deviation from that expected value is considered a critical change. Fixed thresholds can be used to specify when a value is too large or too low. See:

<img src="./images/alerting_thresholds.png" width="500"/>

These checks need to be manually configured for specific metrics/probes. The OpMon configuration files will be extended to something like this:

```toml
[alert]

[alert.crash_diffs] # new alert
type = "threshold"
precentiles = [50, 90] # percentiles that should be considered
min = [0, 0]          # upper threshold for each percentile [optional]
max = [10, 50]        # lower threshold for each percentile [optional]
probes = [      # list of metrics to check
    "startup_crashes", 
    "main_crashes", 
    "oom_crashes"
]
```

An `[alert]` section can be added to configuration files as well as definition files to specify alerts.
The minimum and maximum threshold value as well as the metrics the check should be applied to can be specified.
A list of percentiles needs to be provided that are being checked.

### Deviation from historical data

Users want to get notified if a metric changed unexpectedly, for example after a new version got released. See:

<img src="./images/alerting_historical_diff.png" width="500"/>

It is not always possible to define a specific threshold, instead previously recorded data should be used to detect significant deviations.

This check is the most complicated and computation-intensive one with potentially the highest number of false positives. There are a lot of different anomaly detection algorithms out there, but for OpMon the idea is to detect these anomalies using the exponentially weighted moving average (EWMA):

```math
EWMA_t = a * v_t + (1 - a) * EWMA_{t-1}
```

* `a`: configurable weight; larger values result in more recent recorded values having a larger impact on the moving average (= they will have a higher weight)
* `v`: value of a specific metric
* `EWMA_0`: the starting point, which is equal to the first value recorded for the metric
* `t`: observation day

EWMA considers all historic values but puts a lot of weight on more recent values, older values are given less weight. This should ensure that spikes recorded in the past are less likely to trigger future alerts.

Control limits are determined by calculating the standard deviation:

```math
stdev(v)_t = \sqrt(EWMA(v^2)_t - EWMA(v)^2_t)
```

If the recorded values for a metric exceeds `EWMA_t + stdev(v)_t` or is below `EWMA_t - stdev(v)_t`, then a change is present.

This method is memory and CPU efficient and can be easily implemented in SQL. It only works for metrics with no seasonality. In the case of OpMon most operational metrics, like memory consumption or different load times, are not influenced by different week days or holidays.

A few drawbacks of this methods are that over time the control limits can get temporarily inflated after a spike and changes could go unnoticed afterwards. Slow changes will go unnoticed, for example if a metric slowly degrades over a long period of time, the average will adjust to it instead of noticing the change.
Another issue is that for computing the percentiles for each metric, values need to get bucketed. Depending on the resolution of these buckets, value changes do not show up as steady changes but instead as sudden jumps. This could potentially trigger this check if the resolution of the bucketes is low.

This check needs to be configured for metrics like:

```toml
[alert]

[alert.crash_changes] # new alert
type = "ema"
precentiles = [50, 90] # percentiles that should be considered
weight = 0.4    # weight factor    
probes = [      # list of metrics to check
    "startup_crashes", 
    "main_crashes", 
    "oom_crashes"
]
```


## Displaying Changes and Notifications

Significant changes are displayed in a table on the OpMon dashboard. For example:

<img src="./images/alerting_table.png" width="500"/>

Users can opt-in to receiving notifications via email by creating a new alert on the table in Looker:

<img src="./images/alerting_notification.png" width="500"/>



## Alternatives Considered

### More Advanced Time Series Modelling

Instead of using EWMA there are more advanced models, such as ARIMA models, which can be used to express more complicated characteristicts of time series. The idea is to fit or train these models to sample data and then use them to make predictions or draw conclusions on whether a significant change of certain metrics has occured. 

BigQuery has [support for creating ARIMA models](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-time-series), however the price for using BigQuery ML is quite significant ($250 per TB of processed data). A separate model would be needed for every OpMon project and every metric that uses this type of alerting which would either result in significant cost or computation time (is we, for example, use an external Python library instead).

### Comparing Windows of Averages

The performance team often compares the average value of a metric of the past 7 days to the average value of the 7 days before. Usually, they look at the relative difference and define a threshold which indicates a significant change.

This approach is easy to implement in SQL, however the downside is that whenever spikes happen, alerts will be sent even after the spike has gone down since it will inflate the averages for at least 14 days.


## Implementation

OpMon configuration files and configuration parsing logic needs to be extended to allow configuring the alerts described above.

For each OpMon project a new view `operational_monitoring.<opmon_slug>_alerting` will be automatically generated that implements the checks. The view will look something like this:

```sql
-- A UDF for computing the exponentially weighted moving average (EWMA)
CREATE TEMP FUNCTION ewma(arr ARRAY<FLOAT64>, alpha FLOAT64) 
RETURNS STRUCT<ewma FLOAT64, stdev FLOAT64>
LANGUAGE js AS r"""
  function ewma(a, alpha) {
    var emaArray = [a[0]];
    var varianceArray = [a[0]*a[0]];
    for (var i = 1; i < a.length; i++) {
      emaArray.push(a[i] * alpha + emaArray[i - 1] * (1 - alpha));
      varianceArray.push(a[i]*a[i] * alpha + varianceArray[i - 1] * (1 - alpha));
    }
    return {
      "ewma": emaArray[emaArray.length - 1],
      "stdev": Math.sqrt(varianceArray[varianceArray.length - 1] - Math.pow(emaArray[emaArray.length - 1],2))
     };
  }
  return ewma(arr, alpha);
""";


WITH measured_values AS (
  -- get all scalar and histogram value for each day; group by metric and branch
  SELECT
    submission_date,
    probe,
    branch,
    STRUCT(
        mozfun.hist.merge(
          ARRAY_AGG(
            bug_1751307_pref_tab_unloading_on_low_memory_for_linux_release_97_98_histogram.histogram IGNORE NULLS
          )
        ).values AS values
    ) AS values
  FROM operational_monitoring.bug_1751307_pref_tab_unloading_on_low_memory_for_linux_release_97_98_histogram
  GROUP BY
    submission_date,
    probe,
    branch
  UNION ALL
  SELECT
    submission_date,
    probe,
    branch,
    STRUCT<values ARRAY<STRUCT<key FLOAT64, value FLOAT64>>>(mozfun.map.sum(
        ARRAY_AGG(
            STRUCT<key FLOAT64, value FLOAT64>(
                SAFE_CAST(COALESCE(value, 0.0) AS FLOAT64), 1
            )
        )
    )) AS values
  FROM operational_monitoring.bug_1751307_pref_tab_unloading_on_low_memory_for_linux_release_97_98_scalar
  GROUP BY
    submission_date,
    probe,
    branch
),

ci_overlaps AS (
  -- check if confidence intervals between branches overlap  
  SELECT
    measured_values.submission_date,
    measured_values.probe,
    measured_values.branch,
    ((udf_js.jackknife_percentile_ci(percentile, ref.values).high <= udf_js.jackknife_percentile_ci(percentile, measured_values.values).high AND udf_js.jackknife_percentile_ci(percentile, measured_values.values).low <= udf_js.jackknife_percentile_ci(percentile, ref.values).high) OR
    (udf_js.jackknife_percentile_ci(percentile, measured_values.values).low <= udf_js.jackknife_percentile_ci(percentile, ref.values).low AND udf_js.jackknife_percentile_ci(percentile, ref.values).low <= udf_js.jackknife_percentile_ci(percentile, measured_values.values).high) OR
    (udf_js.jackknife_percentile_ci(percentile, measured_values.values).high <= udf_js.jackknife_percentile_ci(percentile, ref.values).high AND udf_js.jackknife_percentile_ci(percentile, ref.values).low <= udf_js.jackknife_percentile_ci(percentile, measured_values.values).high) OR
    (udf_js.jackknife_percentile_ci(percentile, ref.values).low <= udf_js.jackknife_percentile_ci(percentile, measured_values.values).low AND udf_js.jackknife_percentile_ci(percentile, measured_values.values).low <= udf_js.jackknife_percentile_ci(percentile, ref.values).high)) AS ci_overlap,
    percentile
  FROM measured_values, 
    UNNEST([50]) AS percentile  -- percentiles to check
  JOIN measured_values AS ref
  ON measured_values.submission_date = ref.submission_date AND
    measured_values.branch != ref.branch AND
    measured_values.probe = ref.probe
  WHERE ref.branch = "tab-unloading-disabled" -- reference branch
),

ewma AS (
  -- compute EWMA  
  SELECT
    submission_date,
    probe,
    branch,
    udf_js.jackknife_percentile_ci(50, values) as ci,
    ewma(ARRAY_AGG(udf_js.jackknife_percentile_ci(50, values).percentile) OVER (PARTITION BY branch, probe ORDER BY submission_date DESC), 0.49) AS ewma_percentile
  FROM measured_values
)

-- checks for thresholds
SELECT
  submission_date,
  probe,
  branch,
  50 AS percentile,
  "Value below threshold" AS message
FROM measured_values
WHERE 
  udf_js.jackknife_percentile_ci(50, values).high <= 22 AND probe = "gc_max_pause_2"
-- UNION ALL
-- ...

-- checks for differences in CI
UNION ALL
SELECT
  submission_date,
  probe,
  branch,
  percentile,
  "Significant difference between branches" AS message
FROM ci_overlaps
WHERE 
  probe IN ('gc_ms', 'gc_max_pause_2_content', 'js_pageload_delazification_ms')  -- list of probes check is done for
  AND ci_overlap = FALSE

-- checks for significant changes
UNION ALL
SELECT
    submission_date,
    probe,
    branch,
    50 AS percentile,
    "Significant difference to historical data" AS message
FROM ewma 
WHERE probe IN ("gc_ms")
  AND (ewma_percentile.ewma - ewma_percentile.stdev > ci.high OR ewma_percentile.ewma + ewma_percentile.stdev < ci.low)
```

The query currently does not account for external configurations, but can be easily changed to incorporate those.



## Resources

* https://assets.dynatrace.com/content/dam/en/wp/Anomaly-Detection-for-Monitoring-Ruxit.pdf
